{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadanje modela od prije za pca graf (vecina koda se ponavlja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"/kaggle/working/saved_model\"\n",
    "tokenizer_save_path = \"/kaggle/working/saved_tokenizer\"\n",
    "classifier_save_path = \"/kaggle/working/saved_classifier\"\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model_loaded = TFAutoModel.from_pretrained(model_save_path)\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b44593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize data with reduced max_length\n",
    "def tokenize(batch):\n",
    "    return tokenizer_loaded(batch[\"text_combined\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "english_speaking_countries = ['gb']\n",
    "data_dir = \"/kaggle/input/data-all\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    country_code = file_path.split('-')[1]  # Extract country code from filename\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', quoting=3, on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if country_code in english_speaking_countries:\n",
    "        df['text_combined'] = df['text']\n",
    "    else:\n",
    "        df['text_combined'] = df['text_en']\n",
    "    # Drop rows where 'text_combined' is NaN or empty\n",
    "    df = df.dropna(subset=['text_combined'])\n",
    "    df = df[df['text_combined'] != '']\n",
    "    df['file_path'] = file_path  # Add file path information\n",
    "    return df[['text_combined', 'label', 'file_path']]\n",
    "\n",
    "\n",
    "# Function to split data within each country\n",
    "def train_val_test_split_country(df, test_size=0.2, val_size=0.1):\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=val_size/(1-test_size), random_state=42)\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206eb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from all files, splitting within each country first\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = load_data(file_path)\n",
    "        if not df.empty:\n",
    "            train_df, val_df, test_df = train_val_test_split_country(df)\n",
    "            train_dfs.append(train_df)\n",
    "            val_dfs.append(val_df)\n",
    "            test_dfs.append(test_df)\n",
    "\n",
    "# Concatenate all country-specific splits\n",
    "train_data = pd.concat(train_dfs, ignore_index=True)\n",
    "val_data = pd.concat(val_dfs, ignore_index=True)\n",
    "test_data = pd.concat(test_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset object\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "train_encoded = train_dataset.map(tokenize, batched=True)\n",
    "val_encoded = val_dataset.map(tokenize, batched=True)\n",
    "test_encoded = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for TensorFlow\n",
    "train_encoded.set_format('tf', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label', 'file_path'])\n",
    "val_encoded.set_format('tf', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label', 'file_path'])\n",
    "test_encoded.set_format('tf', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label', 'file_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e65ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to TensorFlow format\n",
    "def to_tf_dataset(encoded_dataset, batch_size):\n",
    "    def generator():\n",
    "        for example in encoded_dataset:\n",
    "            yield ({'input_ids': example['input_ids'],\n",
    "                    'attention_mask': example['attention_mask'],\n",
    "                    'token_type_ids': example['token_type_ids']}, \n",
    "                   example['label'], example['file_path'])\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator,\n",
    "                                          output_signature=(\n",
    "                                              (\n",
    "                                                  {\n",
    "                                                      'input_ids': tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "                                                      'attention_mask': tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "                                                      'token_type_ids': tf.TensorSpec(shape=(128,), dtype=tf.int32)\n",
    "                                                  },\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.int64),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "                                              )\n",
    "                                          )).batch(batch_size)\n",
    "\n",
    "BATCH_SIZE = 64 \n",
    "\n",
    "# Convert datasets to TensorFlow format\n",
    "train_dataset = to_tf_dataset(train_encoded, BATCH_SIZE).shuffle(1000)\n",
    "val_dataset = to_tf_dataset(val_encoded, BATCH_SIZE)\n",
    "test_dataset = to_tf_dataset(test_encoded, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Extract country code from file path\n",
    "def extract_country_code(file_path):\n",
    "    match = re.search(r'orientation-([a-z]{2}(?:-[a-z]{2})?)-train.tsv', file_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    print(f\"No match found for: {file_path}\")\n",
    "    return None\n",
    "\n",
    "# Add country code to the data\n",
    "train_data['country_code'] = train_data['file_path'].apply(extract_country_code)\n",
    "\n",
    "# Function to get BERT representations for the train data\n",
    "def get_bert_representations(dataset):\n",
    "    representations = []\n",
    "    labels = []\n",
    "    countries = []\n",
    "    for batch in dataset:\n",
    "        inputs, label, file_paths = batch\n",
    "        inputs = {key: tf.convert_to_tensor(val) for key, val in inputs.items()}\n",
    "        outputs = model_loaded(inputs)[0]  # Get the last hidden state\n",
    "        mean_representation = tf.reduce_mean(outputs, axis=1)\n",
    "        representations.extend(mean_representation.numpy())\n",
    "        labels.extend(label.numpy())\n",
    "        countries.extend([extract_country_code(fp.numpy().decode('utf-8')) for fp in file_paths])\n",
    "    return np.array(representations), np.array(labels), np.array(countries)\n",
    "\n",
    "# Get BERT representations for the train data\n",
    "train_representations, train_labels, train_file_paths = get_bert_representations(train_dataset)\n",
    "\n",
    "# Calculate mean representations for each country and label\n",
    "mean_representations = {}\n",
    "for country in np.unique(train_file_paths):\n",
    "    for label in [0, 1]:\n",
    "        country_label_indices = (train_file_paths == country) & (train_labels == label)\n",
    "        if np.sum(country_label_indices) > 0:\n",
    "            mean_rep = np.mean(train_representations[country_label_indices], axis=0)\n",
    "            mean_representations[(country, label)] = mean_rep\n",
    "\n",
    "# Prepare data for PCA\n",
    "mean_reps = np.array(list(mean_representations.values()))\n",
    "countries_labels = list(mean_representations.keys())\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(mean_reps)\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for i, ((country, label), pca_coord) in enumerate(zip(countries_labels, pca_result)):\n",
    "    color = 'red' if label == 0 else 'blue'\n",
    "    plt.scatter(pca_coord[0], pca_coord[1], color=color)\n",
    "    \n",
    "    \n",
    "# Connect the points with lines and add country codes\n",
    "for country in np.unique([c for c, l in countries_labels]):\n",
    "    left_coords = pca_result[[i for i, (c, l) in enumerate(countries_labels) if c == country and l == 0]]\n",
    "    right_coords = pca_result[[i for i, (c, l) in enumerate(countries_labels) if c == country and l == 1]]\n",
    "    if len(left_coords) > 0 and len(right_coords) > 0:\n",
    "        plt.plot([left_coords[0][0], right_coords[0][0]], [left_coords[0][1], right_coords[0][1]], 'k-')\n",
    "        mid_x = (left_coords[0][0] + right_coords[0][0]) / 2\n",
    "        mid_y = (left_coords[0][1] + right_coords[0][1]) / 2\n",
    "        plt.text(mid_x, mid_y, country, fontsize=12, color='black', ha='center', va='center')\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA of Mean Representations for Left-Wing and Right-Wing Speeches by Country')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242b525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
