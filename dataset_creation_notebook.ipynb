{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                ids: List[str], \n",
    "                speakers: List[str], \n",
    "                sexes: List[str], \n",
    "                texts: List[str], \n",
    "                texts_en: List[str], \n",
    "                labels: List[bool],\n",
    "                device: torch.device = torch.device('cpu'),\n",
    "                model_name: str = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                max_length: int = 512\n",
    "        ):\n",
    "        assert len(ids) == len(speakers) == len(sexes) == len(texts) == len(texts_en) == len(labels)\n",
    "        self.ids = []\n",
    "        self.speakers = []\n",
    "        self.sexes = []\n",
    "        self.texts = []\n",
    "        self.texts_en = []\n",
    "        self.embeddings = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            inputs = self.tokenizer(texts[i], add_special_tokens=True, return_tensors='pt', padding='max_length',max_length=max_length)\n",
    "            if inputs['input_ids'].shape[1] <= max_length:\n",
    "                inputs = self.tokenizer(texts_en[i], add_special_tokens=True, return_tensors='pt', padding='max_length',max_length=max_length)\n",
    "                self.ids.append(ids[i])\n",
    "                self.speakers.append(speakers[i])\n",
    "                self.sexes.append(sexes[i])\n",
    "                self.texts.append(texts[i])\n",
    "                self.texts_en.append(texts_en[i])\n",
    "                self.embeddings.append(inputs['input_ids'][0])\n",
    "                self.attention_masks.append(inputs['attention_mask'])\n",
    "                self.labels.append(torch.tensor((labels[i]), dtype=torch.long))\n",
    "                \n",
    "        print(f'Loaded {len(self.ids)}/{len(ids)} samples.')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index], self.speakers[index], self.sexes[index], self.texts[index], \\\n",
    "                self.texts_en[index], self.embeddings[index][:512].to(self.device), self.attention_masks[index][0][:512].to(self.device), self.labels[index]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def set_device(self, device: torch.device):\n",
    "        '''\n",
    "        Sets the device to the given device.\n",
    "        '''\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dataset import MyDataset\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "english_speaking_countries = ['gb']\n",
    "\n",
    "#DATA_DIR = \"/kaggle/input/data-all\"\n",
    "DATA_DIR = \"data/orientation\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "OUTPUT_DIR = \"data/torch/orientation\"\n",
    "\n",
    "def load_data(file_path: str):\n",
    "    '''\n",
    "    Loads specified dataset and returns lists of columns\n",
    "    '''\n",
    "    country_code = file_path.split('-')[1]  # Extract country code from filename\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', quoting=3, on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if country_code in english_speaking_countries:\n",
    "        df['text_combined'] = df['text']\n",
    "    else:\n",
    "        df['text_combined'] = df['text_en']\n",
    "    # Drop rows where 'text_combined' is NaN or empty\n",
    "    df = df.dropna(subset=['text_combined'])\n",
    "    df = df[df['text_combined'] != '']\n",
    "    df['file_path'] = file_path  # Add file path information\n",
    "    return list(df['id']), list(df['speaker']), list(df['sex']), list(df['text']), list(df['text_combined']), list(df['label'])\n",
    "\n",
    "def train_val_test_split_country(data: MyDataset, val_size:float = 0.1, test_size:float = 0.1, random_state:int = 42):\n",
    "    train_size = 1 - test_size - val_size\n",
    "    train_data, val_data, test_data = random_split(data, [train_size, val_size, test_size], \\\n",
    "                                                   generator=torch.Generator().manual_seed(random_state))\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = [], [], []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        ids, speakers, sexes, texts, texts_en, labels = load_data(file_path)\n",
    "        df = MyDataset(\n",
    "            ids=ids,\n",
    "            speakers=speakers,\n",
    "            sexes=sexes,\n",
    "            texts=texts,\n",
    "            texts_en=texts_en,\n",
    "            labels=labels,\n",
    "            device=DEVICE,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "        train_df, val_df, test_df = train_val_test_split_country(df)\n",
    "        train_dataset.append(train_df)\n",
    "        val_dataset.append(val_df)\n",
    "        test_dataset.append(test_df)\n",
    "        torch.save(train_df, os.path.join(OUTPUT_DIR, f\"train_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        torch.save(val_df, os.path.join(OUTPUT_DIR, f\"val_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        torch.save(test_df, os.path.join(OUTPUT_DIR, f\"test_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        breakpoint()\n",
    "        print(f\"Processed {filename}, created train, val, and test datasets of size {len(train_df)}, {len(val_df)}, and {len(test_df)} respectively.\")\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_dataset)\n",
    "val_dataset = torch.utils.data.ConcatDataset(val_dataset)\n",
    "test_dataset = torch.utils.data.ConcatDataset(test_dataset)\n",
    "\n",
    "torch.save(train_dataset, os.path.join(OUTPUT_DIR, \"train_dataset_all.pt\"))\n",
    "torch.save(val_dataset, os.path.join(OUTPUT_DIR, \"val_dataset_all.pt\"))\n",
    "torch.save(test_dataset, os.path.join(OUTPUT_DIR, \"test_dataset_all.pt\"))\n",
    "\n",
    "print(f\"Processed all files, created train, val, and test datasets of size {len(train_dataset)}, {len(val_dataset)}, and {len(test_dataset)} respectively.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
