{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerko/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import MyDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('data/power/power-hr-train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sex</th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr10338</td>\n",
       "      <td>accc1a779eb3de6e72a1918eae210a7c</td>\n",
       "      <td>M</td>\n",
       "      <td>Gospodine predsjedniče, uvaženi kolega zastupn...</td>\n",
       "      <td>Mr. President, distinguished counterpart Leko ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr10339</td>\n",
       "      <td>9406c621a816cc966509965a4c0bc023</td>\n",
       "      <td>M</td>\n",
       "      <td>Poštovani gospodine predsjedniče, kolegice i k...</td>\n",
       "      <td>Dear Mr. President, colleagues and colleagues....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hr10340</td>\n",
       "      <td>0b6bca6455fd10bdb7490e7bddfe0ca2</td>\n",
       "      <td>M</td>\n",
       "      <td>Štovani gospodine potpredsjedniče, kolegice i ...</td>\n",
       "      <td>Honored Mr. Vice President, colleagues and coe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hr10341</td>\n",
       "      <td>7c65f68cbec0f780f8878ee435e08820</td>\n",
       "      <td>M</td>\n",
       "      <td>Cijenjeni predsjedniče, cijenjene dame i gospo...</td>\n",
       "      <td>The esteemed President, the esteemed ladies an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hr10342</td>\n",
       "      <td>accc1a779eb3de6e72a1918eae210a7c</td>\n",
       "      <td>M</td>\n",
       "      <td>Hvala lijepo gospodine predsjedniče Hrvatskoga...</td>\n",
       "      <td>Thank you very much, Mr. President of the Croa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10736</th>\n",
       "      <td>hr21074</td>\n",
       "      <td>4f3f32bac32e23425cef32dc5f39a468</td>\n",
       "      <td>M</td>\n",
       "      <td>Pa evo iako sa dosta dobrim dijelom ovog se sl...</td>\n",
       "      <td>Well, here's the thing, even though with a pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10737</th>\n",
       "      <td>hr21075</td>\n",
       "      <td>9687420c53b3ae0fdf1ef0ae0f5e6a69</td>\n",
       "      <td>M</td>\n",
       "      <td>Poštovani potpredsjedniče &lt;PARTY&gt;-a, poštovane...</td>\n",
       "      <td>Dear vice president of &lt;PARTY&gt;, respected coll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>hr21076</td>\n",
       "      <td>0ab354e5a1a18c9585c59347c401c64f</td>\n",
       "      <td>M</td>\n",
       "      <td>Hvala lijepa g. potpredsjedniče. Poštovani g. ...</td>\n",
       "      <td>Thank you very much, Mr. Vice President. Mr Se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>hr21077</td>\n",
       "      <td>69cc60ac1e142a1533592406409e506d</td>\n",
       "      <td>M</td>\n",
       "      <td>Hvala lijepo. Uvaženi predsjedniče &lt;PARTY&gt;-a, ...</td>\n",
       "      <td>Thank you very much. The Honorable President o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10740</th>\n",
       "      <td>hr21078</td>\n",
       "      <td>a8888d06bef14d39e82d933c368e9ca1</td>\n",
       "      <td>M</td>\n",
       "      <td>Poštovani predsjedniče, cijenjene kolegice i k...</td>\n",
       "      <td>Dear President, esteemed colleagues and collea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10741 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                           speaker sex  \\\n",
       "0      hr10338  accc1a779eb3de6e72a1918eae210a7c   M   \n",
       "1      hr10339  9406c621a816cc966509965a4c0bc023   M   \n",
       "2      hr10340  0b6bca6455fd10bdb7490e7bddfe0ca2   M   \n",
       "3      hr10341  7c65f68cbec0f780f8878ee435e08820   M   \n",
       "4      hr10342  accc1a779eb3de6e72a1918eae210a7c   M   \n",
       "...        ...                               ...  ..   \n",
       "10736  hr21074  4f3f32bac32e23425cef32dc5f39a468   M   \n",
       "10737  hr21075  9687420c53b3ae0fdf1ef0ae0f5e6a69   M   \n",
       "10738  hr21076  0ab354e5a1a18c9585c59347c401c64f   M   \n",
       "10739  hr21077  69cc60ac1e142a1533592406409e506d   M   \n",
       "10740  hr21078  a8888d06bef14d39e82d933c368e9ca1   M   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Gospodine predsjedniče, uvaženi kolega zastupn...   \n",
       "1      Poštovani gospodine predsjedniče, kolegice i k...   \n",
       "2      Štovani gospodine potpredsjedniče, kolegice i ...   \n",
       "3      Cijenjeni predsjedniče, cijenjene dame i gospo...   \n",
       "4      Hvala lijepo gospodine predsjedniče Hrvatskoga...   \n",
       "...                                                  ...   \n",
       "10736  Pa evo iako sa dosta dobrim dijelom ovog se sl...   \n",
       "10737  Poštovani potpredsjedniče <PARTY>-a, poštovane...   \n",
       "10738  Hvala lijepa g. potpredsjedniče. Poštovani g. ...   \n",
       "10739  Hvala lijepo. Uvaženi predsjedniče <PARTY>-a, ...   \n",
       "10740  Poštovani predsjedniče, cijenjene kolegice i k...   \n",
       "\n",
       "                                                 text_en  label  \n",
       "0      Mr. President, distinguished counterpart Leko ...      1  \n",
       "1      Dear Mr. President, colleagues and colleagues....      0  \n",
       "2      Honored Mr. Vice President, colleagues and coe...      0  \n",
       "3      The esteemed President, the esteemed ladies an...      1  \n",
       "4      Thank you very much, Mr. President of the Croa...      1  \n",
       "...                                                  ...    ...  \n",
       "10736  Well, here's the thing, even though with a pre...      0  \n",
       "10737  Dear vice president of <PARTY>, respected coll...      1  \n",
       "10738  Thank you very much, Mr. Vice President. Mr Se...      1  \n",
       "10739  Thank you very much. The Honorable President o...      0  \n",
       "10740  Dear President, esteemed colleagues and collea...      0  \n",
       "\n",
       "[10741 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('distilbert/distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerko/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerko/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                ids: List[str], \n",
    "                speakers: List[str], \n",
    "                sexes: List[str], \n",
    "                texts: List[str], \n",
    "                texts_en: List[str], \n",
    "                labels: List[bool],\n",
    "                device: torch.device = torch.device('cpu'),\n",
    "                model_name: str = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                max_length: int = 512\n",
    "        ):\n",
    "        assert len(ids) == len(speakers) == len(sexes) == len(texts) == len(texts_en) == len(labels)\n",
    "        self.ids = ids\n",
    "        self.speakers = []\n",
    "        self.sexes = []\n",
    "        self.texts = []\n",
    "        self.texts_en = []\n",
    "        self.embeddings = []\n",
    "        self.labels = []\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=max_length)\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            text = texts[i]\n",
    "            inputs = self.tokenizer(text, return_tensors='pt')\n",
    "            if inputs['input_ids'].shape[1] <= max_length:\n",
    "                self.ids.append(ids[i])\n",
    "                self.speakers.append(speakers[i])\n",
    "                self.sexes.append(sexes[i])\n",
    "                self.texts.append(texts[i])\n",
    "                self.texts_en.append(texts_en[i])\n",
    "                self.embeddings.append(inputs['input_ids'])\n",
    "                self.labels.append(labels[i])\n",
    "                \n",
    "        print(f'Loaded {len(self.ids)}/{len(ids)} samples.')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index], self.speakers[index], self.sexes[index], self.texts[index], \\\n",
    "                self.texts_en[index], self.embeddings[index].to(self.device), self.labels[index]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def set_device(self, device: torch.device):\n",
    "        '''\n",
    "        Sets the device to the given device.\n",
    "        '''\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "table\n",
    "ids = table['id'].tolist()\n",
    "speakers = table['speaker'].tolist()\n",
    "sexes = table['sex'].tolist()\n",
    "texts = table['text'].tolist()\n",
    "texts_en = table['text_en'].tolist()\n",
    "labels = table['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1462 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14306/14306 samples.\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(ids, speakers, sexes, texts, texts_en, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hr10341',\n",
       " '064e8aa666f3d2c35a3a475103873041',\n",
       " 'M',\n",
       " 'Poštovani gospodine predsjedniče Hrvatskog sabora, poštovani gospodine predsjedniče hrvatske Vlade, dame i gospodo. Ja sam htio svoje pitanje uputiti premijeru gospodinu dr. Ivi Sanaderu ali pošto je on odgovorio na moje pitanje djelomično u odgovoru na pitanje uvaženog zastupnika Bagarića i ja sam isto mislio postaviti pitanje u vezi Statuta grada <PARTY>ara. Pa samo bih molio da mi da svoj ...  \\n\\t\\t\\t\\t\\t\\tkomentar na rezultat, na rezultat izbora koji su bili iz referenduma u <PARTY>aru gdje je 99% kućanstva izrazilo želju za jednim gradom i jednom općinom. Hvala lijepa.',\n",
       " \"Dear Mr. President of the Croatian Parliament, Mr. President of the Croatian Government, ladies and gentlemen. I wanted to refer my question to the Prime Minister Dr. Evey Sanader, but since he answered my question partly in response to the question of the respected representative Bagaric, I also thought to ask a question about the Statute of <PARTY>ar. So I'd just like you to give me your... comment on the result, the result of the elections from the <PARTY>ar referendum where 99% of the households expressed their desire for one city and one municipality. Thank you very much.\",\n",
       " tensor([[  101,  2695,  7103,  3490,  2175, 13102,  7716,  3170,  3653,  5104,\n",
       "           6460,  2094,  8713,  2063, 17850, 22879, 21590,  2290,  7842, 12821,\n",
       "           2050,  1010,  2695,  7103,  3490,  2175, 13102,  7716,  3170,  3653,\n",
       "           5104,  6460,  2094,  8713,  2063, 17850, 22879, 17140, 19163,  2063,\n",
       "           1010,  8214,  1045,  2175, 13102,  7716,  2080,  1012, 14855,  3520,\n",
       "           1044,  3775,  2080, 17917, 29147,  2063,  6770,  2319,  6460,  2039,\n",
       "          21823,  3775, 26563, 28418,  2121,  2226,  2175, 13102,  7716,  2378,\n",
       "           2226,  2852,  1012,  4921,  2072,  2624,  9648,  6820,  4862,  2695,\n",
       "           2080, 15333,  2006,  1051,  2094,  3995, 14550,  3695,  6583,  9587,\n",
       "           6460,  6770,  2319,  6460,  6520, 18349,  7712,  3630,  1057,  1051,\n",
       "           2094,  3995, 14550,  2226,  6583,  6770,  2319,  6460, 23068, 10936,\n",
       "          16515,  2290, 23564,  3367,  6279,  8238,  2050,  4524,  8486,  3540,\n",
       "           1045, 14855,  3520, 21541,  2080, 28616, 12798,  2695, 18891,  3775,\n",
       "           6770,  2319,  6460,  1057,  2310,  5831, 28093, 13210, 24665,  8447,\n",
       "           1026,  2283,  1028, 19027,  1012,  6643,  3520,  2080, 12170,  2232,\n",
       "           9587, 12798,  4830,  2771,  4830, 17917, 29147,  1012,  1012,  1012,\n",
       "          12849,  3672,  2906,  6583,  2128,  9759, 24458,  2102,  1010,  6583,\n",
       "           2128,  9759, 24458,  2102,  1045,  2480, 12821,  2050, 12849,  4478,\n",
       "          10514, 12170,  3669,  1045,  2480,  9782,  2050,  1057,  1026,  2283,\n",
       "           1028, 12098,  2226,  1043,  2094,  6460, 15333,  5585,  1003, 13970,\n",
       "          26642,  9189,  2050,  1045,  2480, 20409, 22360, 27838,  2140,  9103,\n",
       "          23564, 24401,  3490,  2213, 24665,  9365,  2213,  1045, 24401,  3630,\n",
       "           2213,  6728, 21081,  2213,  1012,  1044, 10175,  2050,  5622,  6460,\n",
       "           4502,  1012,   102]]),\n",
       " 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
