{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8615111,"sourceType":"datasetVersion","datasetId":5156236},{"sourceId":8631273,"sourceType":"datasetVersion","datasetId":5168131},{"sourceId":62004,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51796},{"sourceId":62424,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52133}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-07T08:55:27.530737Z","iopub.execute_input":"2024-06-07T08:55:27.531117Z","iopub.status.idle":"2024-06-07T08:55:28.462040Z","shell.execute_reply.started":"2024-06-07T08:55:27.531084Z","shell.execute_reply":"2024-06-07T08:55:28.461133Z"}}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom typing import List, Union\nfrom transformers import AutoTokenizer, AutoModel\n\nclass MyDataset(Dataset):\n    def __init__(self, \n                ids: List[str], \n                speakers: List[str], \n                sexes: List[str], \n                texts: List[str], \n                texts_en: List[str], \n                labels: List[bool],\n                device: torch.device = torch.device('cpu'),\n                model_name: str = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n                max_length: int = 512\n        ):\n        assert len(ids) == len(speakers) == len(sexes) == len(texts) == len(texts_en) == len(labels)\n        self.ids = []\n        self.speakers = []\n        self.sexes = []\n        self.texts = []\n        self.texts_en = []\n        self.embeddings = []\n        self.attention_masks = []\n        self.labels = []\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        for i in range(len(ids)):\n            text = texts[i]\n            inputs = self.tokenizer(text, add_special_tokens=True, return_tensors='pt', padding='max_length',max_length=max_length)\n            if inputs['input_ids'].shape[1] <= max_length:\n                self.ids.append(ids[i])\n                self.speakers.append(speakers[i])\n                self.sexes.append(sexes[i])\n                self.texts.append(texts[i])\n                self.texts_en.append(texts_en[i])\n                self.embeddings.append(inputs['input_ids'][0])\n                self.attention_masks.append(inputs['attention_mask'])\n                self.labels.append(torch.tensor((labels[i]), dtype=torch.long))\n                \n        print(f'Loaded {len(self.ids)}/{len(ids)} samples.')\n\n    def __getitem__(self, index):\n        return self.ids[index], self.speakers[index], self.sexes[index], self.texts[index], \\\n                self.texts_en[index], self.embeddings[index][:512].to(self.device), self.attention_masks[index][0][:512].to(self.device), self.labels[index]\n            \n    def __len__(self):\n        return len(self.ids)\n\n    def set_device(self, device: torch.device):\n        '''\n        Sets the device to the given device.\n        '''\n        self.device = device","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:26:07.417975Z","iopub.execute_input":"2024-06-07T20:26:07.418982Z","iopub.status.idle":"2024-06-07T20:26:12.796897Z","shell.execute_reply.started":"2024-06-07T20:26:07.418945Z","shell.execute_reply":"2024-06-07T20:26:12.795781Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import types\nmy_module = types.ModuleType('dataset', 'jerko')\nmy_module.__dict__.update({'MyDataset': MyDataset})\nimport sys\nsys.modules['dataset'] = my_module\n#MyDataset.__module__ = 'dataset'\ndataset_train = torch.load('/kaggle/input/political-orientation-short/train_dataset_all.pt')\ndataset_valid = torch.load('/kaggle/input/political-orientation-short-en/train_dataset_all.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:26:12.798956Z","iopub.execute_input":"2024-06-07T20:26:12.799474Z","iopub.status.idle":"2024-06-07T20:26:35.233886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom typing import List, Union\nfrom transformers import AutoTokenizer, AutoModel, PreTrainedModel\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertForSequenceClassification\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ndef evaluate(dataset: Dataset, model: PreTrainedModel, device: torch.device = torch.device('cpu'), plot: bool = False):\n    '''\n    Evaluates the model on the given dataset.\n    \n    Parameters:\n        dataset: Dataset\n            The dataset to evaluate on.\n        model: PreTrainedModel\n            The model to evaluate.\n        device: torch.device\n            The device to use.\n        plot: bool\n    '''\n    #model.to(device)\n    model.eval()\n    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n    correct_labels = []\n    model_predictions = []\n    probs = []\n    attentions = []\n    embeddings = []\n    texts = []\n    with torch.no_grad():\n        for batch in loader:\n            id_, speaker, sex, text, text_en, embedding, attention_mask, label = batch\n            texts.extend(text_en)\n            embedding = embedding.to(device)\n            attention_mask = attention_mask.to(device).squeeze(1)\n            assert(attention_mask.nonzero().size() == embedding.nonzero().size())\n            label = label.to(device)\n            model_output = model(input_ids=embedding, labels=label, attention_mask=attention_mask, output_attentions=True)\n            embeddings.extend(embedding.cpu())\n            \n            \n            attention = torch.mean(model_output.attentions[-1], dim=1).squeeze()[:,0]\n           \n           \n            attentions.extend(attention.cpu().numpy())\n            logits = model_output.logits\n            \n            prob = torch.max(torch.softmax(logits, dim=1), dim=1)\n            \n            probs.extend(prob.values.cpu())\n            predictions = torch.argmax(logits, dim=1)\n            correct_labels.extend(label.cpu().numpy())\n            model_predictions.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(correct_labels, model_predictions)\n    print(f'Accuracy: {accuracy}')\n    print(f'Confusion matrix:\\n{confusion_matrix(correct_labels, model_predictions)}')\n    \n    return correct_labels, model_predictions, probs, attentions, embeddings, texts\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:26:35.243272Z","iopub.execute_input":"2024-06-07T20:26:35.243616Z","iopub.status.idle":"2024-06-07T20:26:37.856232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nmodel = torch.load('/kaggle/input/ideology_bert_en/pytorch/1/1/jerko_batica_cased_en.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:26:37.857335Z","iopub.execute_input":"2024-06-07T20:26:37.857791Z","iopub.status.idle":"2024-06-07T20:26:41.942668Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c428eeeebd4489981c8429c4518065"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44bbefac8524fd2b7a063b8b9a5a978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309d952b5f2f4cf0a5cf999f51af3bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e73050b2546486dbb4c333133c2dfaf"}},"metadata":{}}]},{"cell_type":"code","source":"\noutput = evaluate(dataset_valid,model, 'cuda:0')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:26:41.943764Z","iopub.execute_input":"2024-06-07T20:26:41.944093Z","iopub.status.idle":"2024-06-07T20:35:43.117657Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.9611919391296043\nConfusion matrix:\n[[12547   210]\n [ 1009 17645]]\n","output_type":"stream"}]},{"cell_type":"code","source":"labels, predictions, probs, attentions, embeddings, old_texts = output","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:56:08.782122Z","iopub.execute_input":"2024-06-07T20:56:08.783023Z","iopub.status.idle":"2024-06-07T20:56:08.788522Z","shell.execute_reply.started":"2024-06-07T20:56:08.782977Z","shell.execute_reply":"2024-06-07T20:56:08.787267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"labels = np.array(labels)\npredictions = np.array(predictions)\nprobs = np.array(probs)\nattentions = np.array(attentions)\nembeddings = np.array(embeddings)\nprint(attentions.shape)\nprint(attentions.reshape(len(labels),-1).shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:56:11.689521Z","iopub.execute_input":"2024-06-07T20:56:11.689938Z","iopub.status.idle":"2024-06-07T20:56:12.229809Z","shell.execute_reply.started":"2024-06-07T20:56:11.689895Z","shell.execute_reply":"2024-06-07T20:56:12.228698Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(31411, 512)\n(31411, 512)\n","output_type":"stream"}]},{"cell_type":"code","source":"ind = labels == predictions\nprint(len(ind))\nlabels = labels[ind]\npredictions = predictions[ind]\nprobs = probs[ind]\nattentions = attentions[ind]\nembeddings = embeddings[ind]\ntexts = []\nfor i, cond in enumerate(ind):\n    if cond:\n        texts.append(old_texts[i])\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-07T20:56:15.419977Z","iopub.execute_input":"2024-06-07T20:56:15.420378Z","iopub.status.idle":"2024-06-07T20:56:15.503138Z","shell.execute_reply.started":"2024-06-07T20:56:15.420350Z","shell.execute_reply":"2024-06-07T20:56:15.502139Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"31411\n","output_type":"stream"}]},{"cell_type":"code","source":"most_left = torch.topk(torch.tensor(probs), 5000, largest=False).indices.numpy()\ngood = most_left[6]\nmost_important = torch.topk( torch.tensor(attentions[good]), 15).indices.numpy()\n\nprint(tokenizer.decode(embeddings[good][most_important]))\nfor ind in most_left[6:10]:\n    print(texts[ind])\nmost_important = torch.topk( torch.tensor(attentions[most_left]), 5).indices.numpy()\nprint(most_important.shape)\nprint(most_left.shape)\n\ncounter_l = {}\nfor ind, att in zip(most_left, most_important):\n    words = tokenizer.decode(embeddings[ind][att]).lower().split(' ')\n    for word in words:\n        if '[SEP]' in word:\n            word = word.replace('[SEP]', '')\n        if word not in counter_l:\n            counter_l[word] = 1\n        else:\n            counter_l[word] += 1\n            \nprint\nmost_popular_left = list(map(lambda x: x,sorted(counter_l.items(), key=lambda x: -x[1])))\n\nprint(most_popular_left[:100])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T21:10:35.285249Z","iopub.execute_input":"2024-06-07T21:10:35.285690Z","iopub.status.idle":"2024-06-07T21:10:35.485718Z","shell.execute_reply.started":"2024-06-07T21:10:35.285656Z","shell.execute_reply":"2024-06-07T21:10:35.484617Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"##gration here immigrantsam worker Mad [SEP] labour, Mi Recently will want system people\nMadam President, I would like Recently, we have been able to read many stories about labour immigrants who are to be expelled because of small mistakes, which have often been made by previous employers, and things that have happened several years ago. <p> This is basically a new practice from the Migration Court, which now means that thousands of people will have their cases tried in a completely new way. It is about people who behave themselves, pay taxes and contribute to society, people who just want to live their lives here. <p> Abuse of the system should, of course, be prosecuted and countered, but this is not about abuse. These are small mistakes, and there is no way whatsoever of correcting them afterwards. This affects the worker very hard.\n3. To ask the hon. Member for Perth and North Perthshire, representing the House of Commons Commission, whether the Commission plans to further restrict access to the House of Commons in response to the outbreak of covid-19. <p> I tabled this question when there were rumours that Parliament would be closed and that we might not be invited back after the recess. Things have now moved rapidly beyond that, and we have to appreciate the members of staff who support us in keeping this place open so that we can do the vital job of holding the Government to account during this crucial period. May I invite the Minister to pay tribute to those staff who support us?\nI'm asking to talk. <p> CHIAVAROLI (<PARTY>). Mr President, Amendment 2.11 (text 2) is similar to Amendment 3.0.1, which we have also put aside. Therefore, I would ask the rapporteur to give his opinion on both and the Presidency if anything to put them to the vote together. PRESIDENT. - The debate is Excuse me, Senator Chiavaroli, but I do not see any overlap between the two amendments. CHIAVAROLI (<PARTY>). Only the textual wording is different, but it is the same extension and the same time. The approximation of the courts of Abruzzo is extended by three years. The rule is only written differently. PRESIDENT. - The debate is closed. Senator, but the figures also look different. I would ask the rapporteur to comment on this.\nI would like to comment on this, Mr President. The reason for me to talk about this is a sentence that I have literally quoted from the pleading. Then you may find that it does not fall within the scope of this debate; there may be differences of opinion. But it does come from one of the documents on the agenda today. So that's why I'm filing the motion today. I understand that I might make a different assessment than you would, but it's what I did. <p> I understand that very well, and if we're going to work like this with each other in the future, I'd be happy to deal with that, but I'd like us all to do that. That is also a bit of the question I sometimes have, because I have taken one specific sentence out of the document that is on the agenda and I have tabled a motion on it. I think I'll enforce the motion now, and then maybe next time I'll pay more attention to that. I hope we'll all be tilting together, or it'll depend on one or two people here. I don't think that's how it works either.\n(5000, 5)\n(5000,)\n[('[sep]', 2965), ('minister', 213), ('croatia', 206), ('i', 177), ('is', 174), ('mad', 172), ('croatian', 169), ('estonia', 160), ('party', 147), ('the', 145), ('president', 138), ('our', 123), ('estonian', 123), ('##am', 122), ('people', 119), ('we', 112), ('government', 110), ('norway', 107), ('danish', 93), ('state', 89), ('has', 89), ('you', 84), ('eu', 84), ('here', 81), ('now', 77), ('my', 77), ('country', 75), ('are', 72), ('norwegian', 70), ('workers', 69), ('local', 65), ('chairman', 64), ('national', 63), ('this', 60), ('czech', 54), (\"'\", 51), ('municipalities', 50), ('christian', 50), ('netherlands', 49), ('[sep]am', 48), ('deputies', 47), ('portuguese', 46), ('he', 44), ('right', 44), ('waitress', 44), ('serbia', 44), ('[cls]', 43), ('left', 41), ('children', 41), ('republic', 41), ('swedish', 41), ('ho', 40), ('have', 40), ('chamber', 40), ('article', 39), ('northern', 39), ('progress', 38), ('agriculture', 36), ('sweden', 36), ('health', 35), ('federal', 35), ('scottish', 35), ('[sep]z', 35), ('denmark', 35), ('scotland', 35), ('european', 34), ('will', 31), ('##z', 30), ('van', 30), ('farmers', 29), ('dear', 28), ('b', 27), ('she', 27), ('dutch', 26), ('ireland', 26), ('-', 25), ('portugal', 25), ('tallinn', 25), ('young', 24), ('s', 24), ('lords', 24), ('still', 23), ('ladies', 23), ('municipality', 23), ('##inge', 23), ('police', 23), ('gentleman', 23), ('energy', 23), ('your', 23), ('socialist', 23), ('of', 23), ('green', 22), ('a', 22), ('[sep]y', 22), ('no', 22), ('secretary', 21), ('france', 21), ('labour', 21), ('austria', 21), ('[sep]!', 21)]\n","output_type":"stream"}]},{"cell_type":"code","source":"most_right = torch.topk(torch.tensor(probs), 5000, largest=True).indices.numpy()\n\nmost_important = torch.topk( torch.tensor(attentions[most_right]), 5).indices.numpy()\n\n\n\n\n\ncounter_r = {}\nfor ind, att in zip(most_right, most_important):\n    words = tokenizer.decode(embeddings[ind][att]).lower().split(' ')\n    \n    for word in words:\n        if '[SEP]' in word:\n            word = word.replace('[SEP]', '')\n        if word not in counter_r:\n            counter_r[word] = 1\n        else:\n            counter_r[word] += 1\n            \n\nmost_popular_right = list(map(lambda x: x,sorted(counter_r.items(), key=lambda x: -x[1])))\nprint(most_popular_right[:100])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = 'treasury'\nprint(counter_l[w])\nprint(counter_r[w])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}